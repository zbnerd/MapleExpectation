# ADR-043: TieredCache (L1 Caffeine + L2 Redis)와 Single-flight 패턴

## 제1장: 문제의 발견 (Problem)

### 초기 상황

MapleExpectation은 2025년 12월경 Redis 단일 계층 캐시로 운영되었습니다. 당시 시스템은 다음과 같은 성능 문제를 겪고 있었습니다.

**Evidence (EVIDENCE-006)**: `wrk` 부하 테스트 결과
- 최대 RPS: 719
- P50 지연시간: 45ms
- P99 지연시간: 450ms

**발견된 문제:**

1. **Redis 지연시간**: 네트워크 왕복(RTT) + 직렬화/역직렬화로 인해 10-20ms 지연
2. **Cache Stampede**: 캐시 만료 시 다수 요청이 동시에 DB/API를 조회하는 Thundering Herd 현상
3. **Redis 부하**: 모든 조회가 Redis를 거쳐 100% Redis 부하
4. **Single-flight 미구현**: 동일 키에 대한 100개 동시 요청이 100번의 API 호출 발생

### 증거: Cache Stampede 현상

**Evidence (EVIDENCE-004)**: `service/v2/EquipmentService.java:119-150`

```java
// 100개 동시 요청이 같은 캐릭터 장비 조회
// L1 MISS -> L2 MISS -> 100개 동일 Nexon API 호출
// Nexon Rate Limit (500 RPS) 중 20%를 단일 캐릭터가 소비
```

이로 인해 다음과 같은 문제가 발생했습니다:
- 인기 캐릭터 조회 시 단일 캐릭터가 전체 Rate Limit의 20% 소비
- API 오버로드로 인한 전체 서비스 지연
- 캐시 만료 시점마다 DB/API에 급격한 부하 증가

---

## 제2장: 선택지 탐색 (Options)

### 선택지 1: Redis-only 캐시 (Status Quo)

| 항목 | 설명 |
|------|------|
| **구현** | 현재 상태 유지 - Redis 단일 계층 |
| **장점** | 구현 단순, 데이터 일관성 보장 |
| **단점** | 지연시간 10-20ms, 100% Redis 부하, Cache Stampede 위험 |
| **적합성** | ❌ 저지연 요구사항 미달 |

### 선택지 2: Cache-aside 패턴 (Single-flight 없음)

| 항목 | 설명 |
|------|------|
| **구현** | 애플리케이션에서 캐시 관리, 캐시 Miss 시 DB 조회 |
| **장점** | 구현 간단, Spring @Cacheable 지원 |
| **단점** | Cache Stampede 방지 불가, 동시 요청 시 중복 DB 조회 |
| **적합성** ❌ Thundering Herd 문제 미해결 |

### 선택지 3: Write-through 캐시

| 항목 | 설명 |
|------|------|
| **구현** | 캐시와 DB에 동시에 기록, 읽기는 캐시에서만 |
| **장점** | 데이터 일관성 높음 |
| **단점** | 쓰기 지연시간 40% 증가 (DB + Redis 직렬화) |
| **적합성** ❌ 쓰기 성능 저하로 인한 전체 RPS 감소 |

### 선택지 4: TieredCache + Single-flight (채택)

| 항목 | 설명 |
|------|------|
| **구현** | L1 Caffeine (Local, 5분 TTL) + L2 Redis (Distributed, 10분 TTL) + Single-flight |
| **장점** | L1 히트 시 <1ms 지연, Redis 부하 87% 감소, Cache Stampede 완전 방지 |
| **단점** | 힙 메모리 제약, 분산 환경에서 Single-flight 미작동 (P0 기술 부채) |
| **적합성** ✅ 저지연 + 고성능 요구사항 충족 |

---

## 제3장: 결정의 근거 (Decision)

### 채택: TieredCache + Single-flight 패턴

**결정 근거:**

1. **성능 요구사항 충족**
   - L1 Cache Hit Rate: 85-95% (N01 Thundering Herd Test)
   - L1 Hit 시 지연시간: <1ms (Caffeine In-Memory)
   - Redis 부하 감소: 87% (L1이 대부분 요청 처리)

2. **Cache Stampede 방지**
   - Single-flight 패턴으로 100개 동시 요청 → 1개 API 호출
   - **Evidence (N01 Test)**: Single-flight 적용 전 100 API 호출 → 적용 후 1 API 호출

3. **GZIP 압축으로 Redis 메모리 절감**
   - 90% 압축률로 Redis 메모리 사용량 감소
   - L2는 압축된 데이터 저장, L1은 압축 해제 후 사용

4. **Spring @Cacheable(sync=true) 호환**
   - Spring Framework 공식 권장 패턴
   - 동일 키 동시 요청 시 1회만 계산

### 트레이드오프 분석

| 항목 | 이점 | 비용 |
|------|------|------|
| **성능** | L1 85-95% hit rate → <1ms 응답 | 힙 메모리 제한 (~500MB) |
| **Redis 부하** | L1이 87% Redis 요청 감소 | L2→L1 쓰기 순서 준수 필요 |
| **Cache Stampede** | Single-flight로 Thundering Herd 방지 | In-Memory 상태로 분산되지 않음 (P0) |
| **데이터 일관성** | L1 TTL ≤ L2 TTL로 L2가 항상 Superset | L2 실패 시 L1 저장 스킵 필요 |

---

## 제4장: 구현의 여정 (Action)

### 4.1 TieredCache 아키텍처

**Evidence Location**: `/docs/03_Technical_Guides/infrastructure.md` (Section 17)

**2계층 캐시 구조:**

```
┌─────────────────────────────────────────────────────────────┐
│  L1: Caffeine (Local In-Memory, 5분 TTL, ~500MB)           │
│  - ConcurrentHashMap 기반                                   │
│  - Hit Rate: 85-95%                                         │
│  - Response: <1ms                                           │
├─────────────────────────────────────────────────────────────┤
│  L2: Redis (Distributed, 10분 TTL, GZIP 압축)              │
│  - Redisson 3.27.0                                          │
│  - Hash Tag 패턴 (Cluster 호환성)                           │
│  - Response: 10-20ms                                        │
├─────────────────────────────────────────────────────────────┤
│  Loader: Nexon Open API (5-10초)                           │
│  - Single-flight 패턴으로 중복 호출 방지                     │
└─────────────────────────────────────────────────────────────┘
```

### 4.2 쓰기 순서: L2 → L1 (원자성 보장)

**Code Evidence** (infrastructure.md:315-318):

```java
// Good (L2 먼저 저장, 실패 시 L1 스킵)
public void put(String key, ValueWrapper value) {
    boolean l2Success = l2Cache.put(key, value);  // L2 먼저
    if (l2Success) {
        l1Cache.put(key, value);  // L2 성공 시에만 L1 저장
    }
}

// Bad (L1 먼저 저장, L2 실패 시 불일치)
public void put(String key, ValueWrapper value) {
    l1Cache.put(key, value);  // L1 먼저
    l2Cache.put(key, value);  // L2 실패 시 L1에만 데이터 존재 -> 불일치
}
```

**필수 규칙:**
- **필수**: L2(Redis) 저장 성공 후에만 L1(Caffeine) 저장
- **금지**: L1 먼저 저장 후 L2 저장 (L2 실패 시 불일치 발생)
- **L2 실패 시**: L1 저장 스킵, 값은 반환 (가용성 유지)

### 4.3 Single-flight 패턴 구현

**Code Evidence** (infrastructure.md:348-352):

```java
// Leader-Follower 패턴
public V get(String key, Callable<V> loader) {
    // 1. L1 캐시 확인
    V value = l1Cache.get(key);
    if (value != null) return value;

    // 2. 분산 락 획득 (Redisson RLock)
    RLock lock = redissonClient.getLock("cache:lock:" + key);
    boolean acquired = lock.tryLock(30, TimeUnit.SECONDS);

    if (acquired) {
        // Leader: Double-check L2 -> Loader 실행 -> L2 저장 -> L1 저장
        try {
            value = l2Cache.get(key);
            if (value == null) {
                value = loader.call();  // 1회만 실행
                l2Cache.put(key, value);
                l1Cache.put(key, value);
            }
            return value;
        } finally {
            lock.unlock();
        }
    } else {
        // Follower: L2에서 읽기 -> L1 Backfill
        value = l2Cache.get(key);
        if (value != null) {
            l1Cache.put(key, value);
        }
        return value;
    }
}
```

**핵심 메커니즘:**
- **Leader**: 락 획득 → Double-check L2 → valueLoader 실행 → L2 저장 → L1 저장
- **Follower**: 락 대기 → L2에서 읽기 → L1 Backfill
- **락 실패 시**: Fallback으로 직접 실행 (가용성 우선)

### 4.4 Redis Lua Script & Hash Tag 패턴

**Code Evidence** (infrastructure.md:58-111):

**Lua Script 원자적 연산:**

```java
// Good (원자적 RENAME + EXPIRE + HGETALL)
private static final String LUA_ATOMIC_MOVE = """
        local exists = redis.call('EXISTS', KEYS[1])
        if exists == 0 then return {} end
        redis.call('RENAME', KEYS[1], KEYS[2])
        redis.call('EXPIRE', KEYS[2], ARGV[1])
        return redis.call('HGETALL', KEYS[2])
        """;

RScript script = redissonClient.getScript(StringCodec.INSTANCE);
List<Object> result = script.eval(
        RScript.Mode.READ_WRITE,          // 데이터 변경 시
        LUA_ATOMIC_MOVE,
        RScript.ReturnType.MULTI,         // 복수 결과 반환 시
        Arrays.asList(sourceKey, tempKey), // KEYS[1], KEYS[2]
        String.valueOf(ttlSeconds)         // ARGV[1]
);
```

**Redis Cluster Hash Tag 규칙 (CRITICAL):**

```java
// Bad (다른 해시값 -> Cluster에서 실패)
String sourceKey = "buffer:likes";
String tempKey = "buffer:likes:sync:uuid";

// Good (Hash Tag -> 같은 슬롯 보장)
String sourceKey = "{buffer:likes}";
String tempKey = "{buffer:likes}:sync:" + UUID.randomUUID();
```

**Hash Tag 적용 대상:**
- **RENAME 키 쌍**: `{domain}:source` ↔ `{domain}:target`
- **Lua Script 다중 키**: 모든 KEYS는 같은 Hash Tag
- **MGET/MSET 키들**: 같은 Hash Tag 사용

### 4.5 Graceful Degradation 패턴

**Code Evidence** (infrastructure.md:391-416):

```java
// Bad (Redis 장애 시 예외 전파 -> 서비스 장애)
boolean acquired = lock.tryLock(30, TimeUnit.SECONDS);

// Good (Graceful Degradation -> 가용성 유지)
boolean acquired = executor.executeOrDefault(
        () -> lock.tryLock(30, TimeUnit.SECONDS),
        false,  // Redis 장애 시 락 획득 실패로 처리 -> Fallback 실행
        TaskContext.of("Cache", "AcquireLock", keyStr)
);
```

**적용 대상 (4곳):**
| 위치 | 래핑 대상 | 기본값 |
|------|----------|--------|
| `getCachedValueFromLayers()` | L2.get() | null |
| `executeWithDistributedLock()` | lock.tryLock() | false |
| `executeDoubleCheckAndLoad()` | L2.get() (Double-check) | null |
| `unlockSafely()` | lock.unlock() | null |

### 4.6 Micrometer 메트릭

**Code Evidence** (infrastructure.md:353-359):

```java
// 메트릭 명명 규칙 (소문자 점 표기법)
meterRegistry.counter("cache.hit", "layer", "L1").increment();
meterRegistry.counter("cache.miss").increment();
meterRegistry.counter("cache.lock.failure").increment();
meterRegistry.counter("cache.l2.failure").increment();
```

**필수 메트릭 항목:**
| 메트릭 | 용도 |
|--------|------|
| `cache.hit{layer=L1/L2}` | 캐시 히트율 모니터링 |
| `cache.miss` | Cache Stampede 빈도 확인 |
| `cache.lock.failure` | 락 경합 상황 감지 |
| `cache.l2.failure` | Redis 장애 감지 |

---

## 제5장: 결과와 학습 (Result)

### 현재 상태 (2026-01-28 기준)

**성과:**

| 항목 | Before | After | 개선율 |
|------|--------|-------|--------|
| **L1 Hit Rate** | 0% | 85-95% | +85-95%p |
| **P50 지연시간** | 45ms | <5ms | 89% 감소 |
| **Redis 부하** | 100% | 13% | 87% 감소 |
| **API 중복 호출** | 100회 (100요청) | 1회 | 99% 감소 |

**Evidence (N01 Thundering Herd Test):**
- Single-flight 없음: 100요청 → 100 API 호출
- Single-flight 있음: 100요청 → 1 API 호출

**잘 된 점:**

1. **저지연 응답**: L1 Hit 시 <1ms로 Redis-only 캐시 대비 10-20배 개선
2. **Redis 부하 감소**: 87% 감소로 Redis 장애 내성 확보
3. **Thundering Herd 방지**: Single-flight로 캐시 만료 시점의 급격한 부하 방지
4. **Spring @Cacheable 호환**: sync=true 모드로 Spring Framework 공식 패턴 준수
5. **Graceful Degradation**: Redis 장애 시에도 서비스 가용성 유지

### 아쉬운 점 및 기술 부채

**P0 기술 부채: Single-flight가 분산되지 않음**

**Evidence (scale-out-blockers-analysis.md:187-202)**:

```java
// P0-4: SingleFlightExecutor — In-Memory inFlight Map
private final ConcurrentHashMap<String, InFlightEntry<T>> inFlight = new ConcurrentHashMap<>();
```

| 항목 | 내용 |
|------|------|
| **문제** | 진행 중인 계산을 인스턴스 메모리에만 저장. 인스턴스 B에 동일 요청 시 2번 계산 |
| **영향** | Single-flight 효과 상실, 계산 중복 N배, API 오버로드 |
| **해결** | Redis 기반 Distributed Single-Flight 구현 (`single-flight:{keyHash}` + TTL) |

**현재 한계:**
1. **In-Memory 상태**: Single-flight의 `inFlight` Map이 JVM 로컬에만 존재
2. **Scale-out 불가**: 2개 인스턴스 시 동일 키에 대해 각각 API 호출
3. **Scale-out 방해 요소**: P0-4로 분류됨 (Evidence: EVIDENCE-S005)

**해결 방향 (Sprint 2):**
- Redis 기반 Distributed Single-Flight 구현
- Redis Key: `single-flight:{keyHash}`
- TTL: 30초 (API 타임아웃보다 길게)
- Lua Script로 원자적 획득/해제

### 추가 학습 사항

**TTL 규칙:**
- **필수**: L1 TTL ≤ L2 TTL (L2가 항상 Superset)
- **이유**: L2 먼저 만료되면 L1에만 데이터 존재 → 불일치

**GZIP 압축:**
- L2 Redis에 압축된 데이터 저장 (90% 압축률)
- L1 Caffeine에 압축 해제된 데이터 저장
- Redis 메모리 사용량 절감 + 네트워크 전송량 감소

**Hash Tag 패턴:**
- Redis Cluster에서 다중 키 연산 필수
- `{key}` 패턴으로 같은 슬롯 보장
- 약 15% 메모리 오버헤드지만 Cluster 호환성 확보

### 향후 개선 계획

**Sprint 2 (2026-02-28 예정):**
- [ ] P0-4: SingleFlightExecutor → Redis Distributed Single-Flight
- [ ] L1/L2 TTL 동기화 자동화
- [ ] Cache Warm-up 전략 수립

**Sprint 3 (2026-03-15 예정):**
- [ ] Multi-region 캐시 일관성
- [ ] Cache Invalidation 방식 개선 (Event-driven)
- [ ] L1 캐시 크기 동적 조절

---

## 관련 문서

- **Infrastructure Guide**: [Section 17](../03_Technical_Guides/infrastructure.md#17-tieredcache--cache-stampede-prevention)
- **Scale-out Blockers Analysis**: [P0-4 SingleFlightExecutor](../05_Reports/04_09_Scale_Out/scale-out-blockers-analysis.md#p0-4-singleflightexecutor---in-memory-inflight-map)
- **High Traffic Performance Analysis**: [P0-4 Cache Stampede](../05_Reports/04_02_Cost_Performance/high-traffic-performance-analysis.md#p0-4-cache-stampede-equipmentresponse-레벨)
- **Test Strategy**: [TC-P01 TieredCache Single-flight Test](../02_Chaos_Engineering/00_Overview/TEST_STRATEGY.md#p0-필수-테스트-목록)

---

## 검증 가능성 (Verifiability)

**코드 검증:**

```bash
# TieredCache L1/L2 구조 확인
grep -r "Caffeine\|ConcurrentHashMap" src/main/java/maple/expectation/global/cache/

# Single-flight 락 패턴 확인
grep -r "RLock\|tryLock" src/main/java/maple/expectation/global/cache/

# Hash Tag 패턴 확인
grep -r "{.*}:" src/main/java/maple/expectation/global/cache/

# Graceful Degradation 패턴 확인
grep -r "executeOrDefault" src/main/java/maple/expectation/global/cache/
```

**성능 검증:**

```bash
# 캐시 히트율 확인
curl -s http://localhost:8080/actuator/metrics/cache.gets | jq '.measurements'

# L1 Hit Rate 확인
curl -s http://localhost:8080/actuator/metrics/cache.hit | jq '.measurements[] | select(.statistic=="COUNT")'

# P50 지연시간 확인
wrk -t4 -c100 -d30s --latency http://localhost:8080/api/v4/character/test/expectation
```

---

*A 작성일: 2026-02-19*
*A 상태: Accepted*
*A 검증 버전: v1.2.0*
*A 다음 리뷰: 2026-03-19*
