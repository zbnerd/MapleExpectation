# ADR-058: L1 Local Cache로 Caffeine 3.1.8 채택

## 상태 (Status)
**Accepted** (2026-02-19)

## 문맥 (Context)
- **카테고리**: Technology Stack
- **관련 컴포넌트**: TieredCache, Infrastructure
- **영향 범위**: 전체 캐시 계층 (L1 Local Cache)

---

## 제1장: 문제의 발견 (Problem)

### 1.1 Redis-only 캐시의 성능 한계

2025-12 말, 프로덕션 환경에서 **Redis network latency**가 병목으로 확인되었습니다.

- **Network Round-trip**: 10-20ms (AWS t3.small → ElastiCache)
- **Hot Key 집중**: 87% 요청이 동일한 10% 키에 집중 (장비 조회, 확률 데이터)
- **Cache Stampede 위험**: L2 만료 시 다수 요청이 동시에 Redis 접근

### 1.2 분석 데이터 (Evidence)

`docs/05_Reports/04_02_Cost_Performance/high-traffic-performance-analysis.md` 분석 결과:

| 지표 | 측정값 |
|------|--------|
| Redis 평균 latency | 15ms |
| Hot Key 요청 비율 | 87% |
| L2 cache hit rate | 92% |
| Network Round-trip | 10-20ms |

**문제 요약**: Hot key에 대한 반복 Redis query는 불필요한 network overhead를 유발하며, Redis cluster에 부하를 집중시킵니다.

### 1.3 결정 필요성

1,000+ RPS 목표 달성을 위해 **<1ms local cache**가 필수적이었습니다.

---

## 제2장: 선택지 탐색 (Options)

### 2.1 대안 1: Guava Cache

| 장점 | 단점 |
|------|------|
| 안정적 (Google 유지보수) | Caffeine의 전신, 성능 낮음 |
| Spring Boot 기본 지원 | Window TinyLru 구조로 hit rate 낮음 |
| 단순한 API | 비동기 로딩 미지원 |

**벤치마크**: Caffeine 대비 25-30% 낮은 hit rate (Caffeine官方 spec)

### 2.2 대안 2: Ehcache 3.x

| 장점 | 단점 |
|------|------|
| Disk persist 지원 | 복잡한 설정 (XML/Programmatic) |
| Tiered Cache 기본 지원 | 디스크 기능 불필요 (in-memory만 사용) |
| JSR-107 표준 준수 | Spring Boot 3.x auto-configuration 미약 |

**평가**: In-memory만 사용할 예정이므로 디스크 기능은 over-engineering.

### 2.3 대안 3: ConcurrentHashMap 직접 구현

| 장점 | 단점 |
|------|------|
| 의존성 없음 | TTL/eviction 로직 직접 구현 부담 |
| 완전한 제어 | Cache statistics 수동 구현 |
| 가벼움 | Thread-safe 동시성 로직 버그 위험 |

**평가**: Cache Stampede 방지를 위한 sync 로딩, eviction 전략, statistics 등 직접 구현 시 **버그 위험 > 이점**.

### 2.4 대안 4: Caffeine 3.1.8 (선정)

| 장점 | 단점 |
|------|------|
| **최고 hit rate** (Window TinyLfu) | JVM heap 사용 (~500MB) |
| **Spring Cache 3.x 공식 호환** | 분산 환경에서 stale data 위험 |
| **Auto-loading** (`@Cacheable(sync=true)`) | L1 size 제한 (heap 제약) |
| Zero-dependency | |
| Async loading 지원 | |
| Built-in statistics | |

**벤치마크**: [Caffeine官方 Benching](https://github.com/ben-manes/caffeine/wiki/Benchmarks)

| Cache Library | Hit Rate (81 key set) |
|---------------|----------------------:|
| **Caffeine** | **94.6%** |
| Guava | 73.3% |
| Ehcache 2 | 66.3% |
| ConcurrentHashMap | 56.8% |

---

## 제3장: 결정의 근거 (Decision)

### 3.1 최종 선택: Caffeine 3.1.8

**결정 이유**:

1. **Spring Boot 3.x 공식 호환**
   ```groovy
   implementation 'com.github.ben-manes.caffeine:caffeine:3.1.8'
   ```
   Spring Cache `@Cacheable(sync=true)`와 자동 통합으로 sync 로딩 지원.

2. **최고 Hit Rate (Window TinyLfu)**
   - Light eviction: 접근 패턴 학습
   - Hot key 집중 워크로드에 최적화
   - **Guava 대비 21% 향상** (94.6% vs 73.3%)

3. **Auto-loading으로 Cache Stampede 방지**
   ```java
   @Cacheable(cacheNames="equipment", sync=true)
   public Equipment findEquipment(String id) {
       // 동일 키 동시 요청 -> 1회만 계산 (Spring + Caffeine)
   }
   ```

4. **Zero-dependency**
   - 외부 의존성 없음 (Guava는 Google Core Libraries 전체 의존)

### 3.2 거부 대안 거부 이유

| 대안 | 거부 이유 |
|------|----------|
| Guava | Caffeine의 전신, 성능 낮음 (25% hit rate gap) |
| Ehcache | 디스크 기능 불필요, 설정 복잡도 높음 |
| ConcurrentHashMap | 직접 구현 부담, 버그 위험, sync 로딩 직접 구현 |
| Redis-only | Network latency 10-20ms -> 1,000+ RPS 달성 어려움 |

### 3.3 Trade-off 승인

| 항목 | Trade-off | 완화 전략 |
|------|-----------|----------|
| **JVM Heap 사용** | ~500MB 추가 | Aggressive TTL (5분) + size-based eviction |
| **Stale Data 위험** | 분산 환경 L1 불일치 | **L1 TTL ≤ L2 TTL** (L2 Superset 보장) |
| **L1 Size 제한** | Heap 제약으로 캐시 용량 한정 | Hot key만 캐시 (LRU eviction) |

---

## 제4장: 구현의 여정 (Action)

### 4.1 의존성 추가 (build.gradle)

```groovy
// subprojects/global/build.gradle
dependencies {
    implementation 'com.github.ben-manes.caffeine:caffeine:3.1.8'
}
```

**Evidence**: `/home/maple/MapleExpectation/build.gradle:96-116` (subprojects 공통 의존성 설정)

### 4.2 CacheManager 구성 (Spring Cache 통합)

```java
@Configuration
@EnableCaching
public class CacheConfig {

    @Bean
    public CacheManager cacheManager() {
        CaffeineCacheManager cacheManager = new CaffeineCacheManager();
        cacheManager.setCaffeine(Caffeine.newBuilder()
                .maximumSize(10_000)              // LRU eviction
                .expireAfterWrite(5, TimeUnit.MINUTES)  // 5분 TTL
                .recordStats()                    // Built-in statistics
        );
        return cacheManager;
    }
}
```

### 4.3 TieredCache 구현 (L1 + L2)

`docs/03_Technical_Guides/infrastructure.md` Section 17:

```java
// TieredCache.get(key, Callable) 패턴
public ValueWrapper get(Object key, Callable<?> valueLoader) {
    // 1. L1 (Caffeine) 조회
    ValueWrapper wrapper = l1Cache.get(key);
    if (wrapper != null) {
        recordHit("L1");
        return wrapper;  // <1ms response
    }

    // 2. L2 (Redis) 조회
    wrapper = l2Cache.get(key);
    if (wrapper != null) {
        l1Cache.put(key, wrapper.get());  // L1 Backfill
        recordHit("L2");
        return wrapper;  // 10-20ms response
    }

    // 3. Miss -> valueLoader 실행 (Single-flight)
    return loadAndCache(key, valueLoader);
}
```

**Evidence**: `docs/03_Technical_Guides/infrastructure.md:306-417` (Section 17 TieredCache)

### 4.4 Spring @Cacheable(sync=true) 사용

```java
@Service
public class EquipmentService {

    @Cacheable(cacheNames="equipment", sync=true)
    public Equipment findEquipment(String id) {
        // L1 MISS 시 Single-flight로 1회만 실행
        return equipmentRepository.findById(id);
    }
}
```

**sync=true 동작**:
- L1에 동일 키로 100개 동시 요청
- Spring Cache + Caffeine이 1회만 `findEquipment()` 실행
- 나머지 99개 요청은 완료 대기 후 결과 반환

### 4.5 TTL 규칙 (L1 ≤ L2)

```yaml
# application.yml
spring:
  cache:
    caffeine:
      spec: maximumSize=10000,expireAfterWrite=5m  # L1: 5분
    redis:
      time-to-live: 600000  # L2: 10분 (L1 TTL의 2배)
```

**규칙**: L1 TTL(5분) ≤ L2 TTL(10분)으로 L2가 항상 Superset임을 보장.

---

## 제5장: 결과와 학습 (Result)

### 5.1 현재 상태

| 지표 | 결과 |
|------|------|
| **L1 Hit Rate** | 85-95% (N01 Chaos Test) |
| **L1 Response Time** | <1ms (in-memory) |
| **Redis Load Reduction** | 87% |
| **Cache Stampede** | 0건 (sync=true로 방지) |
| **JVM Heap 사용** | ~500MB (t3.small 2GB 환경에서 25%) |

**Evidence**: `docs/03_Technical_Guides/infrastructure.md:306-312`

> "L1 cache reduces Redis load by 87% (Evidence: Performance Report)"

### 5.2 잘 된 점 (Success)

1. **<1ms Response Time**
   - L1 hit 시 0.5-1ms 응답 (Redis 10-20ms 대비 20배 향상)
   - Hot key(장비 조회, 확률 데이터)의 87%를 L1에서 처리

2. **87% Redis Load Reduction**
   - ElastiCost 비용 절감 (Redis Read Units 감소)
   - Network IO 감소로 전체 시스템 여유 확보

3. **Spring @Cacheable(sync=true) 자동 통합**
   - 별도 Single-flight 라이브러리 불필요
   - Spring Framework 공식 패턴으로 유지보수 용이

4. **Built-in Statistics**
   ```java
   CacheStats stats = caffeineCache.stats();
   double hitRate = stats.hitRate();  // 0.94 (94%)
   long missCount = stats.missCount();
   ```

### 5.3 아쉬운 점 (Lessons Learned)

1. **JVM Heap 사용 (~500MB)**
   - t3.small (2GB) 환경에서 25% heap 점유
   - **완화**: Maximum size 제한 + Aggressive TTL

2. **Stale Data 위험 (분산 환경)**
   - Multi-instance 환경에서 L1 불일치 가능성
   - **완화**: L1 TTL ≤ L2 TTL (L2 Superset 보장)

3. **Cold Start 시 L1 Empty**
   - 서버 재시작 후 L1 hit rate 0% 시작
   - **완화**: Warm-up 전략 (별도 ADR 검토 필요)

### 5.4 개선 방안

1. **Warm-up Strategy (ADR-059 제안)**
   - 서버 시작 시 Hot key를 L1에 사전 로딩
   - L1 hit rate를 초기부터 90%+ 유지

2. **L1 Size Tuning**
   - 현재: 10,000 entries
   - 모니터링 후 eviction rate 기반 동적 조정

3. **Metrics Dashboard**
   - L1/L2 hit rate 모니터링 (Prometheus)
   - L1 miss가 20% 초과 시 알람

---

## 관련 문서

- **Section 17**: `docs/03_Technical_Guides/infrastructure.md` - TieredCache 패턴
- **Performance Report**: `docs/05_Reports/04_02_Cost_Performance/high-traffic-performance-analysis.md`
- **Caffeine Benching**: https://github.com/ben-manes/caffeine/wiki/Benchmarks
- **Spring Cache**: https://docs.spring.io/spring-framework/reference/integration/cache.html

---

## 검증 (Validation)

이 ADR은 무효화될 수 있습니다:

1. **L1 Hit Rate < 80%**: Caffeine이 예상만큼 성능을 내지 못함
2. **OOM 발생**: JVM heap 부족으로 t3.small에서 OutOfMemoryError
3. **Stale Data로 P0 장애**: L1/L2 불일치로 데이터 무결성 훼손
4. **Spring 4.x에서 Caffeine 지원 중단**: 대안 재검토 필요

**검증 명령어**:
```bash
# L1 Hit Rate 확인
curl -s http://localhost:8080/actuator/metrics/cache.gets | jq '.measurements[] | select(.statistic=="VALUE")'

# JVM Heap 사용량 확인
curl -s http://localhost:8080/actuator/metrics/jvm.memory.used | jq '.measurements[] | select(.statistic=="VALUE" and .tags.area=="heap")'
```

---

*작성일: 2026-02-19*
*검증 상태: Accepted*
*다음 검토: 2026-03-19*
